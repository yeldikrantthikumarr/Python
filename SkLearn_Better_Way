# Learning  Scikit-learn better way
#These are my practice codes, sourced, collected from various sources (group discussion, meetups, presentations, knowledge sharing platforms, ideas sharing, brain storming sessions, and explored via self-practices)


import sklearn
from sklearn import datasets
boston = datasets.load_boston()
type(boston)
boston.feature_names
boston['data']

type(boston['data'])
from sklearn.ensemble import RandomForestRegressor
clf = RandomForestRegressor()
clf.fit(boston['data'], boston['target'])
clf.score(boston['data'],boston['target'])
clf.score? # the "_" (underscore) variables are learned variables in sklearn
dir(clf)
clf.n_features_
boston['data'].shape
row = boston['data'][17]
row.shape
row.reshape(-1,13)
clf.predict(row.reshape(-1,13))
boston['target'][17]
################################################################################################################################################

# In real world, the developed model has to predict on new data. The underlying risk exisits named as "Overfitting"

# OVerfitting: - In simpler terms, the model is accurate on existing data but fails on new data
# Lets understand common practices in model development:
# Firstly, split the data into training data and test data
# Once the model is built, use test data to evaluate  the model.
# Through this Sklearn the process is very easy

################################################################################################################################################
#Remeber: to split data there are many ways to do it other than X_train,X_test,y_train,y_test ..eg: k-fold
