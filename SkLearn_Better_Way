# Learning  Scikit-learn better way
#These are my practice codes, sourced, collected from various sources (group discussion, meetups, presentations, knowledge sharing platforms, ideas sharing, brain storming sessions, and explored via self-practices)


import sklearn
from sklearn import datasets
boston = datasets.load_boston()
type(boston)
boston.feature_names
boston['data']

type(boston['data'])
from sklearn.ensemble import RandomForestRegressor
clf = RandomForestRegressor()
clf.fit(boston['data'], boston['target'])
clf.score(boston['data'],boston['target'])
clf.score? # the "_" (underscore) variables are learned variables in sklearn
dir(clf)
clf.n_features_
boston['data'].shape
row = boston['data'][17]
row.shape
row.reshape(-1,13)
clf.predict(row.reshape(-1,13))
boston['target'][17]
################################################################################################################################################

# In real world, the developed model has to predict on new data. The underlying risk exisits named as "Overfitting"

# OVerfitting: - In simpler terms, the model is accurate on existing data but fails on new data
# Lets understand common practices in model development:
# Firstly, split the data into training data and test data
# Once the model is built, use test data to evaluate  the model.
# Through this Sklearn the process is very easy

################################################################################################################################################
#Remeber: to split data there are many ways to do it other than X_train,X_test,y_train,y_test ..eg: k-fold

# First understand your data and apply split of data....for eg: For credit card fraud, we hardly have fraud...as we might have fewer fraud points, this equal split of train and test might be an issue.
# Hence, first gothorough your dataset properly and perform split of data into train and test

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(boston['data'],boston['target'],test_size = 0.3)
clf = RandomForestRegressor()
clf.fit(X_train,y_train)
clf.score(X_test, y_test)
# output was 0.8205688983225575  ...i.e 82% accuracy

################################################################################################################################################
# Pre-processing data
# some algorithms are more sensitive to range of feature values
import pandas as pd
df = pd.DataFrame(boston['data'], columns = boston['feature_names'])
df.max(axis = 0) # axis=0 ...gives rows data

from sklearn.svm import SVR
clf = SVR()
clf.fit(X_train, y_train)
clf.score(X_test, y_test)

# scaling all parameters on same scale !!

from sklearn import preprocessing
Xs = preprocessing.scale(boston['data'])
df = pd.DataFrame(Xs, columns = boston['feature_names'])
df.max(axis =0)
# there is no much data after preprocessing/ scaling the data

Xstrain,Xstest, ytrain,ytest = train_test_split(Xs, boston['target'], test_size = 0.3)
#remember when you specify test_size = 0.3 means 30 %. This parameter decides how much % the data has tobe divided. in this case it is 30% test data and the rest is train data

clf = SVR()
clf.fit(Xs_train, ys_train)
clf.score(Xs_test, ys_test)
####################################################################################################################################################################################################################################

#if you are worried about reducing features in the data set, the most popular technique is "dimensionality reduction" (Dimensional reduction)
# one of the known algorithm is PCA (Principal Component Analysis)


from sklearn. decomposition import PCA
pca = PCA(n_components=5)
pca.fit(boston['data'])

Xp = pca.transform(boston['data'])
Xp.shape() # output is 5

clf = RandomForestRegressor()
Xp_train, Xp_test, yp_train, yp_test = train_test_split(Xp, boston['target'],test_size =0.3)
clf.fit(Xp_train, yp_train)
clf.score(Xp_test, yp_test)

#Exploring Pipelines .....



